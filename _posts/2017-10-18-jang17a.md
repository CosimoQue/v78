---
title: End-to-End Learning of Semantic Grasping
abstract: We consider the task of semantic robotic grasping, in which a robot picks
  up an object of a user-specified class using only monocular images. Inspired by
  the two-stream hypothesis of visual reasoning, we present a semantic grasping framework
  that learns object detection, classification, and grasp planning in an end-to-end
  fashion. A “ventral stream” recognizes object class while a “dorsal stream” simultaneously
  interprets the geometric relationships necessary to execute successful grasps. We
  leverage the autonomous data collection capabilities of robots to obtain a large
  self-supervised dataset for training the dorsal stream, and use semi-supervised
  label propagation to train the ventral stream with only a modest amount of human
  supervision. We experimentally show that our approach improves upon grasping systems
  whose components are not learned end-to-end, including a baseline method that uses
  bounding box detection. Furthermore, we show that jointly training our model with
  auxiliary data consisting of non-semantic grasping data, as well as semantically
  labeled images without grasp actions, has the potential to substantially improve
  semantic grasping performance.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: jang17a
month: 0
tex_title: End-to-End Learning of Semantic Grasping
firstpage: 119
lastpage: 132
page: 119-132
order: 119
cycles: false
author:
- given: Eric
  family: Jang
- given: Sudheendra
  family: Vijayanarasimhan
- given: Peter
  family: Pastor
- given: Julian
  family: Ibarz
- given: Sergey
  family: Levine
date: 2017-10-18
address: 
publisher: PMLR
container-title: Proceedings of the 1st Annual Conference on Robot Learning
volume: '78'
genre: inproceedings
issued:
  date-parts:
  - 2017
  - 10
  - 18
pdf: http://proceedings.mlr.press/v78/jang17a/jang17a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
